{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This one is in development - shows Tensorgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline \n",
    "\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper_functions import *\n",
    "\n",
    "Struct = namedtuple('FLAGS', 'learning_rate log_dir dropout max_steps') \n",
    "dir_path = os.path.abspath('')\n",
    "\n",
    "FLAGS = Struct(log_dir=os.path.join(dir_path,'logs'), dropout=0.5, \\\n",
    "               max_steps=15000, learning_rate=1e-4)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "\n",
    "class Number_Data():\n",
    "    def __init__(self, data, filter_type='original'):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0        \n",
    "        self._x_data, self._y_data = collect_images(data, filter_type)                \n",
    "        target = np.zeros((len(self._y_data), 10))\n",
    "        target[np.arange(len(self._y_data)),self._y_data] = 1\n",
    "        self._y_data = target\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self._x_data,self._y_data,random_state=42)\n",
    "        self._num_examples = len(self.x_train)\n",
    "        self.train_data_count = len(self.x_train)\n",
    "        self.test_data_count = len(self.x_test)\n",
    "        \n",
    "        # Flatten training images.\n",
    "        n_samples = len(self.x_train)\n",
    "        self.x_train = self.x_train.reshape((n_samples,-1))\n",
    "        n_samples = len(self.x_test)\n",
    "        self.x_test = self.x_test.reshape((n_samples,-1))        \n",
    "        \n",
    "        # Get flattened test images.\n",
    "        self.test_images = self.test_images()\n",
    "        self.test_labels = self.y_test\n",
    "        \n",
    "        if tf.gfile.Exists(FLAGS.log_dir):\n",
    "            tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "        tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "        \n",
    "    def train(self):\n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        # Initialize input and target tensors.\n",
    "        with tf.name_scope('input'):\n",
    "            x = tf.placeholder(tf.float32, shape=[None, 784],name='x-input')\n",
    "            y_ = tf.placeholder(tf.float32, shape=[None, 10],name='y-input')\n",
    "\n",
    "        # Initialize weight and bias tensors.\n",
    "        with tf.name_scope('convolutional_layer_1'):\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])            \n",
    "            b_conv1 = bias_variable([32])\n",
    "        with tf.name_scope('input_reshape'):\n",
    "            x_image = tf.reshape(x, [-1,28,28,1])\n",
    "            tf.summary.image('input', x_image, 10)\n",
    "            \n",
    "        with tf.name_scope('convolutional_layer1'):\n",
    "            h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "            \n",
    "        with tf.name_scope('pooling_layer_1'):\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "        \n",
    "        with tf.name_scope('convolutional_layer_2'):\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])        \n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        \n",
    "        with tf.name_scope('pooling_layer_2'):\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])            \n",
    "        \n",
    "        h_fc1 = nn_layer(h_pool2_flat, 7*7*64, 512, 'fully_connected_layer_1')\n",
    "        print(h_fc1,h_pool2_flat,h_pool2)\n",
    "        \n",
    "        with tf.name_scope('dropout'):\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)        \n",
    "\n",
    "#         with tf.name_scope('fully_connected_layer_2'):        \n",
    "#             W_fc2 = weight_variable([512, 10])\n",
    "#             b_fc2 = bias_variable([10])\n",
    "\n",
    "        y_conv = nn_layer(h_fc1_drop, 512, 10, 'fully_connected_layer_2')\n",
    "#         y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "                \n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            # The raw formulation of cross-entropy,\n",
    "            #\n",
    "            # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "            #                               reduction_indices=[1]))\n",
    "            #\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "            # raw outputs of the nn_layer above, and then average across\n",
    "            # the batch.\n",
    "            diff = tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_)\n",
    "            with tf.name_scope('total'):\n",
    "                cross_entropy = tf.reduce_mean(diff)\n",
    "\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n",
    "#         cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, self.y_))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        \n",
    "        # Merge all the summaries and write them out to /logs\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train',\n",
    "                                      sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # Train the model, and also write summaries.\n",
    "        # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "        # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "        def feed_dict(train):\n",
    "            \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "            if train:\n",
    "                xs, ys = self.next_batch(300)\n",
    "                k = FLAGS.dropout\n",
    "            else:\n",
    "                xs, ys = self.test_images, self.test_labels\n",
    "                k = 1.0\n",
    "            return {x: xs, y_: ys, keep_prob: k}\n",
    "    \n",
    "        for i in range(FLAGS.max_steps):\n",
    "#             batch = self.next_batch(300)  \n",
    "            if i % 10 == 0:  # Record summaries and test-set accuracy                \n",
    "                summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "                test_writer.add_summary(summary, i)\n",
    "                print('Accuracy at step %s: %s' % (i, acc))\n",
    "            else:  # Record train set summaries, and train\n",
    "                if i % 100 == 99:  # Record execution stats\n",
    "#                     run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "#                     run_metadata = tf.RunMetadata()\n",
    "                    summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "#                     train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                    train_writer.add_summary(summary, i)\n",
    "                    print('Adding run metadata for', i)\n",
    "                else:\n",
    "                    summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                    train_writer.add_summary(summary, i)\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "#         correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self.y_,1))\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def next_batch(self, batch_size=100, shuffle=True):\n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for the first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._images = self.x_train[perm0]\n",
    "            self._labels = self.y_train[perm0]\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            images_rest_part = self.x_train[start:self._num_examples]\n",
    "            labels_rest_part = self.y_train[start:self._num_examples]\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                perm = np.arange(self._num_examples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._images = self._images[perm]\n",
    "                self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            images_new_part = self._images[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return np.concatenate((images_rest_part, images_new_part), axis=0) , np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch            \n",
    "            return self._images[start:end], self._labels[start:end]\n",
    "        \n",
    "    def test_images(self):\n",
    "        images = self.x_test\n",
    "        n_samples = len(images)\n",
    "        return images.reshape((n_samples, -1))\n",
    "\n",
    "#     def train_network(self):\n",
    "#         for i in range(20000):\n",
    "#             batch = self.next_batch(100)    \n",
    "#             if i%100 == 0:\n",
    "#                 train_accuracy = self.accuracy.eval(feed_dict={self.x:batch[0], self.y_: batch[1], self.keep_prob: 1.0})\n",
    "#                 print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "#             self.train_step.run(feed_dict={self.x: batch[0], self.y_: batch[1], self.keep_prob: 0.5})\n",
    "\n",
    "#         print(\"test accuracy %g\"%self.accuracy.eval(feed_dict={\n",
    "#             self.x: self.test_images, self.y_: self.test_labels, self.keep_prob: 1.0}))\n",
    "\n",
    "data = pickle.load(open('data3_4.p', 'rb'))\n",
    "number_data = Number_Data(data, 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected_layer_1/activation:0\", shape=(?, 512), dtype=float32) Tensor(\"pooling_layer_2/Reshape:0\", shape=(?, 3136), dtype=float32) Tensor(\"pooling_layer_2/MaxPool:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py:1391 in _flatten_outer_dims.: concat (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2016-12-13.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.concat_v2().\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py:1391 in _flatten_outer_dims.: concat (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2016-12-13.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.concat_v2().\n",
      "Accuracy at step 0: 0.0678689\n",
      "Accuracy at step 10: 0.146209\n",
      "Accuracy at step 20: 0.0781462\n",
      "Accuracy at step 30: 0.0781462\n",
      "Accuracy at step 40: 0.0781462\n",
      "Accuracy at step 50: 0.0781462\n",
      "Accuracy at step 60: 0.0781462\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4be01eb6684f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumber_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-6160a0add4fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adding run metadata for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                     \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_data.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The one works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py:1391 in _flatten_outer_dims.: concat (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2016-12-13.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.concat_v2().\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py:1391 in _flatten_outer_dims.: concat (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2016-12-13.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.concat_v2().\n",
      "step 0, training accuracy 0.11\n",
      "step 100, training accuracy 0.88\n",
      "step 200, training accuracy 0.943333\n",
      "test accuracy 0.925926\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper_functions import *\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "class Number_Data():\n",
    "    def __init__(self, data, filter_type='original'):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0        \n",
    "        self._x_data, self._y_data = collect_images(data, filter_type)                \n",
    "        target = np.zeros((len(self._y_data), 10))\n",
    "        target[np.arange(len(self._y_data)),self._y_data] = 1\n",
    "        self._y_data = target\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self._x_data,self._y_data,random_state=42)\n",
    "        self._num_examples = len(self.x_train)\n",
    "        self.train_data_count = len(self.x_train)\n",
    "        self.test_data_count = len(self.x_test)\n",
    "        print(self.y_test[0:1])\n",
    "        # Flatten training images.\n",
    "        n_samples = len(self.x_train)\n",
    "        self.x_train = self.x_train.reshape((n_samples,-1))\n",
    "        n_samples = len(self.x_test)\n",
    "        self.x_test = self.x_test.reshape((n_samples,-1))        \n",
    "        \n",
    "        # Get flattened test images.\n",
    "        self.test_images = self.test_images()\n",
    "        self.test_labels = self.y_test\n",
    "        \n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        # Initialize input and target tensors.\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "        # Initialize weight and bias tensors.\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "\n",
    "        x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 512])\n",
    "        b_fc1 = bias_variable([512])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n",
    "\n",
    "        W_fc2 = weight_variable([512, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "\n",
    "        self.y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_))\n",
    "\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y_conv,1), tf.argmax(self.y_,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def __call__(self,feed_list):        \n",
    "        \"\"\" Classify image. \"\"\"\n",
    "        \n",
    "        feed_dict = {self.x : feed_list[0], self.y_: feed_list[1], self.keep_prob:feed_list[2]}\n",
    "#         classification = tf.run(self.y_, feed_dict)\n",
    "#         print(classification)\n",
    "        sess = tf.get_default_session()\n",
    "        classification = sess.run(self.y_conv, feed_dict)\n",
    "        print(classification)\n",
    "        train_accuracy = self.accuracy.eval(feed_dict=feed_dict)\n",
    "        print(\"test accuracy %g\"%train_accuracy)\n",
    "        \n",
    "    def next_batch(self, batch_size=100, shuffle=True):\n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for the first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._images = self.x_train[perm0]\n",
    "            self._labels = self.y_train[perm0]\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            images_rest_part = self.x_train[start:self._num_examples]\n",
    "            labels_rest_part = self.y_train[start:self._num_examples]\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                perm = np.arange(self._num_examples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._images = self._images[perm]\n",
    "                self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            images_new_part = self._images[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return np.concatenate((images_rest_part, images_new_part), axis=0) , np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch            \n",
    "            return self._images[start:end], self._labels[start:end]\n",
    "        \n",
    "    def test_images(self):\n",
    "        images = self.x_test\n",
    "        n_samples = len(images)\n",
    "        return images.reshape((n_samples, -1))\n",
    "\n",
    "    def train_network(self):\n",
    "        for i in range(300):\n",
    "            batch = self.next_batch(300)    \n",
    "            if i%100 == 0:\n",
    "                train_accuracy = self.accuracy.eval(feed_dict={self.x:batch[0], self.y_: batch[1], self.keep_prob: 1.0})\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            self.train_step.run(feed_dict={self.x: batch[0], self.y_: batch[1], self.keep_prob: 0.5})\n",
    "\n",
    "        print(\"test accuracy %g\"%self.accuracy.eval(feed_dict={\n",
    "            self.x: self.test_images, self.y_: self.test_labels, self.keep_prob: 1.0}))\n",
    "\n",
    "data = pickle.load(open('data3_4.p', 'rb'))\n",
    "\n",
    "number_data = Number_Data(data, 'original')\n",
    "number_data.train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]] 0\n",
      "(1, 784)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'classication' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bc6e92ebc66c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnumber_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-71bc6eb689d7>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, feed_list)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassication\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test accuracy %g\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'classication' is not defined"
     ]
    }
   ],
   "source": [
    "# Check individual image classification\n",
    "\n",
    "for i in range(10):\n",
    "    image = data['original'][-i].reshape((-1,784))\n",
    "    target_number = data['targets'][-i]\n",
    "    target = np.zeros((1, 10))\n",
    "    target[np.arange(1),target_number] = 1    \n",
    "    print(target,target_number)\n",
    "    print(image.shape)    \n",
    "    \n",
    "    feed_list = [image, target, 1.0]\n",
    "    number_data(feed_list)\n",
    "    plt.imshow(image.reshape((28,28)),cmap=plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
