{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The one that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justinshenk/Projects/tensorflow/digits-classifier/src\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'visualization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c8ff7659703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'visualization'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib notebook \n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join\n",
    "from helper_functions import *\n",
    "from visualization import Visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "    \n",
    "class Number_Data():\n",
    "    def __init__(self, data, filter_type='original'):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        self.viz = Visualization(epochs=EPOCHS)\n",
    "        self._x_data, self._y_data = collect_images(data, filter_type)                \n",
    "        target = np.zeros((len(self._y_data), 10))\n",
    "        target[np.arange(len(self._y_data)),self._y_data] = 1\n",
    "        self._y_data = target\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self._x_data,self._y_data,random_state=42)\n",
    "        self._num_examples = len(self.x_train)\n",
    "        self.train_data_count = len(self.x_train)\n",
    "        self.test_data_count = len(self.x_test)\n",
    "        \n",
    "        # Flatten training images.\n",
    "        n_samples = len(self.x_train)\n",
    "        self.x_train = self.x_train.reshape((n_samples,-1))\n",
    "        n_samples = len(self.x_test)\n",
    "        self.x_test = self.x_test.reshape((n_samples,-1))        \n",
    "        \n",
    "        # Get flattened test images.\n",
    "        self.test_images = self.get_test_images()\n",
    "        self.test_labels = self.y_test\n",
    "        \n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        # Initialize input and target tensors.\n",
    "        with tf.name_scope('input'):\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 784], name='x-input')\n",
    "            print(self.x)\n",
    "            self.y_ = tf.placeholder(tf.float32, shape=[None, 10], name='y-input')\n",
    "\n",
    "        # Initialize weight and bias tensors.\n",
    "        with tf.name_scope('convolutional_layer_1'):\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            b_conv1 = bias_variable([32])\n",
    "        \n",
    "        with tf.name_scope('input_reshape'):\n",
    "            x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "            tf.summary.image('input', x_image, 10)\n",
    "            \n",
    "        with tf.name_scope('convolutional_layer1'):\n",
    "            h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "            \n",
    "        with tf.name_scope('pooling_layer_1'):\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "        \n",
    "        with tf.name_scope('convolutional_layer_2'):\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "            b_conv2 = bias_variable([64])        \n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        \n",
    "        with tf.name_scope('pooling_layer_2'):\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            \n",
    "        with tf.name_scope('fully_connected_layer_1'):\n",
    "            W_fc1 = weight_variable([7 * 7 * 64, 512])\n",
    "            b_fc1 = bias_variable([512])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        \n",
    "        with tf.name_scope('dropout'):\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "            tf.summary.scalar('dropout_keep_probability', self.keep_prob)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)        \n",
    "\n",
    "        with tf.name_scope('fully_connected_layer_2'):\n",
    "            W_fc2 = weight_variable([512, 10])\n",
    "            b_fc2 = bias_variable([10])\n",
    "            self.y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            # The raw formulation of cross-entropy,\n",
    "            #\n",
    "            # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "            #                               reduction_indices=[1]))\n",
    "            #\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "            # raw outputs of the nn_layer above, and then average across\n",
    "            # the batch.\n",
    "            \n",
    "            diff = tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_)\n",
    "            with tf.name_scope('total'):\n",
    "                self.cross_entropy = tf.reduce_mean(diff)\n",
    "\n",
    "        tf.summary.scalar('cross_entropy', self.cross_entropy)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.cross_entropy)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.argmax(self.y_conv, 1), tf.argmax(self.y_, 1))\n",
    "            with tf.name_scope('accuracy'):\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "        # Merge all the summaries and write them out to /logs\n",
    "        dir_path = os.path.abspath('')\n",
    "        log_dir=join(dir_path,'logs')        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(join(log_dir,'train'),\n",
    "                                      sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(join(log_dir,'test'))\n",
    "        tf.global_variables_initializer().run()\n",
    "            \n",
    "    def next_batch(self, batch_size=100, shuffle=True):\n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for the first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._images = self.x_train[perm0]\n",
    "            self._labels = self.y_train[perm0]\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            images_rest_part = self.x_train[start:self._num_examples]\n",
    "            labels_rest_part = self.y_train[start:self._num_examples]\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                perm = np.arange(self._num_examples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._images = self._images[perm]\n",
    "                self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            images_new_part = self._images[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return np.concatenate((images_rest_part, images_new_part), axis=0) , np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch            \n",
    "            return self._images[start:end], self._labels[start:end]\n",
    "        \n",
    "    def __call__(self,feed_list):        \n",
    "        \"\"\" Classify image. \"\"\"\n",
    "        \n",
    "        feed_dict = {self.x : feed_list[0], self.y_: feed_list[1], self.keep_prob:feed_list[2]}\n",
    "#         classification = tf.run(self.y_, feed_dict)\n",
    "#         print(classification)\n",
    "        sess = tf.get_default_session()\n",
    "        classification = sess.run(self.y_conv, feed_dict)\n",
    "        print(classification)\n",
    "        train_accuracy = self.accuracy.eval(feed_dict=feed_dict)\n",
    "        print(\"test accuracy %g\"%train_accuracy)\n",
    "\n",
    "    def get_test_images(self):\n",
    "        images = self.x_test\n",
    "        n_samples = len(images)\n",
    "        return images.reshape((n_samples, -1))\n",
    "\n",
    "    def train_network(self):\n",
    "        sess = tf.get_default_session()\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            batch = self.next_batch(100)    \n",
    "            summary, _, acc, cross_entropy = sess.run([self.merged, self.train_step, self.accuracy, self.cross_entropy], feed_dict={self.x : batch[0], self.y_ : batch[1], self.keep_prob: 0.5})\n",
    "            \n",
    "            if epoch % 100 == 99: # Record train set summaries, and test                \n",
    "                run_metadata = tf.RunMetadata()\n",
    "                self.train_writer.add_run_metadata(run_metadata, 'step%03d' % epoch)\n",
    "\n",
    "                # Add training summary\n",
    "                self.train_writer.add_summary(summary, epoch)\n",
    "                                \n",
    "                summary, test_accuracy, cross_entropy = sess.run([self.merged, self.accuracy, self.cross_entropy],feed_dict={self.x : self.test_images, self.y_ : self.test_labels, self.keep_prob: 1.0})\n",
    "                print(\"step %d, test accuracy %g\"%(epoch, test_accuracy))\n",
    "                self.viz(epoch, test=test_accuracy, cross_entropy=cross_entropy)\n",
    "                self.test_writer.add_summary(summary, epoch)\n",
    "            \n",
    "            # Visualize training progress every 25 steps.\n",
    "            if epoch % 25 == 0:                            \n",
    "                print(\"step %d, training accuracy %g\"%(epoch, acc))\n",
    "                self.viz(epoch, train=acc, cross_entropy=cross_entropy)\n",
    "          \n",
    "        self.train_writer.close()\n",
    "        self.test_writer.close()\n",
    "\n",
    "# Load dataset.\n",
    "data = pickle.load(open('data3_4.p', 'rb'))\n",
    "\n",
    "number_data = Number_Data(data, 'original')\n",
    "number_data.train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check individual image classification\n",
    "\n",
    "for i in range(10):\n",
    "    image = data['original'][-i].reshape((-1,784))\n",
    "    target_number = data['targets'][-i]\n",
    "    target = np.zeros((1, 10))\n",
    "    target[np.arange(1),target_number] = 1    \n",
    "    print(target,target_number)\n",
    "    print(image.shape)    \n",
    "    \n",
    "    feed_list = [image, target, 1.0]\n",
    "    number_data(feed_list)\n",
    "    plt.imshow(image.reshape((28,28)),cmap=plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
